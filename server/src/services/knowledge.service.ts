import fs from 'fs';
import path from 'path';
import prisma from '../config/database.js';
import { KnowledgeBase, Prisma } from '@prisma/client';
import { uploadToS3 } from '../config/storage.js';
import openai from '../config/openai.js';
import axios from 'axios';

// Interfaces
interface CreateKnowledgeBaseParams {
    name: string;
    description: string;
    sourceLanguage: string;
    targetLanguage: string;
    userId: string;
    originalFileName?: string;
}

interface TextChunk {
    content: string;
    index: number;
    wordCount: number;
}

interface TranslationContext {
    previousChunk?: TextChunk;
    nextChunk?: TextChunk;
    knowledgeBase?: string;
    prompt?: string;
    sourceLanguage: string;
    targetLanguage: string;
}

// Fun√ß√µes principais
export const processKnowledgeBaseFile = async (filePath: string, params: CreateKnowledgeBaseParams): Promise<KnowledgeBase> => {
    console.log('üîÑ [1/7] Iniciando processamento do arquivo:', filePath);

    try {
        // Validar arquivo
        if (!fs.existsSync(filePath)) {
            throw new Error('Arquivo tempor√°rio n√£o encontrado');
        }

        // Ler o arquivo
        console.log('üìñ [2/7] Lendo arquivo');
        const fileContent = await fs.promises.readFile(filePath);
        const uniqueFileName = `kb_${Date.now()}_${path.basename(filePath)}`;
        const displayFileName = params.originalFileName || path.basename(filePath);
        const fileSize = fileContent.length;
        const fileType = path.extname(filePath).slice(1);

        // Validar tipo de arquivo
        const allowedTypes = ['txt', 'csv', 'xlsx', 'xls'];
        if (!allowedTypes.includes(fileType.toLowerCase())) {
            throw new Error(`Tipo de arquivo n√£o suportado. Tipos permitidos: ${allowedTypes.join(', ')}`);
        }

        // Fazer upload para o S3 com nome √∫nico
        console.log('‚òÅÔ∏è [3/7] Enviando para o Spaces');
        const spacesUrl = await uploadToS3(fileContent, uniqueFileName, 'knowledge');
        console.log('‚úÖ [4/7] Upload conclu√≠do:', spacesUrl);

        // Adicionar processamento do conte√∫do
        console.log('üìë [5/7] Processando conte√∫do e gerando embeddings');
        const content = await fs.promises.readFile(filePath, 'utf-8');
        const chunks = splitIntoChunks(content);
        
        // Criar base de conhecimento com chunks
        const knowledgeBase = await prisma.knowledgeBase.create({
            data: {
                name: params.name,
                description: params.description,
                sourceLanguage: params.sourceLanguage,
                targetLanguage: params.targetLanguage,
                fileName: params.originalFileName || path.basename(filePath),
                filePath: spacesUrl,
                fileSize: fileContent.length,
                fileType: path.extname(filePath).slice(1),
                userId: params.userId,
                chunks: {
                    createMany: {
                        data: await Promise.all(chunks.map(async chunk => ({
                            content: chunk,
                            embedding: await getEmbedding(chunk)
                        })))
                    }
                }
            },
            include: {
                chunks: true
            }
        });

        // Limpar arquivo tempor√°rio
        await fs.promises.unlink(filePath);
        console.log('üßπ Arquivo tempor√°rio removido');

        return knowledgeBase;
    } catch (error) {
        console.error('‚ùå Erro ao processar arquivo da base de conhecimento:', error);
        
        // Limpar arquivo tempor√°rio em caso de erro
        try {
            if (fs.existsSync(filePath)) {
                await fs.promises.unlink(filePath);
                console.log('üßπ Arquivo tempor√°rio removido ap√≥s erro');
            }
        } catch (cleanupError) {
            console.error('‚ö†Ô∏è Erro ao limpar arquivo tempor√°rio:', cleanupError);
        }
        
        throw error;
    }
};

// Fun√ß√£o para processar conte√∫do da base de conhecimento
export const getKnowledgeBaseContent = async (knowledgeBaseId: string): Promise<string> => {
    try {
        const knowledgeBase = await prisma.knowledgeBase.findUnique({
            where: { id: knowledgeBaseId }
        });

        if (!knowledgeBase) {
            throw new Error('Base de conhecimento n√£o encontrada');
        }

        const response = await axios.get(knowledgeBase.filePath, {
            headers: {
                'Accept': 'text/plain',
                'Content-Type': 'text/plain'
            }
        });

        return response.data;
    } catch (error) {
        console.error('Erro ao buscar conte√∫do da base de conhecimento:', error);
        throw error;
    }
};

// Fun√ß√£o para traduzir com contexto
export const translateWithContext = async (chunk: TextChunk, context: TranslationContext) => {
    try {
        let relevantContext = '';
        
        if (context.knowledgeBase) {
            try {
                // Buscar apenas o contexto relevante usando embeddings
                relevantContext = await getRelevantKnowledgeBaseContext(
                    chunk.content,
                    context.knowledgeBase,
                    3 // N√∫mero de trechos mais relevantes
                );
            } catch (error) {
                console.error('Erro ao buscar contexto relevante:', error);
            }
        }

        // Montar o prompt otimizado
        const prompt = `
            ${context.prompt || ''}
            
            Contexto Relevante da Base de Conhecimento:
            ${relevantContext}
            
            Texto para traduzir:
            ${chunk.content}
            
            Contexto anterior: ${context.previousChunk?.content || ''}
            Pr√≥ximo contexto: ${context.nextChunk?.content || ''}
            
            Traduza o texto de ${context.sourceLanguage} para ${context.targetLanguage}.
        `.trim();

        const response = await openai.chat.completions.create({
            model: "gpt-4-turbo-preview", // Modelo mais eficiente
            messages: [{ role: "user", content: prompt }],
            temperature: 0.3,
            max_tokens: 4000 // Limitar tokens de sa√≠da
        });

        return response;
    } catch (error) {
        console.error('Erro na tradu√ß√£o:', error);
        throw error;
    }
};

// Fun√ß√£o para buscar contexto relevante usando embeddings
const getRelevantKnowledgeBaseContext = async (query: string, knowledgeBaseId: string, limit: number = 3): Promise<string> => {
    try {
        const queryEmbedding = await getEmbedding(query);
        
        type ChunkResult = {
            content: string;
            similarity: number;
        };

        const relevantChunks = await prisma.$queryRaw<ChunkResult[]>`
            SELECT 
                content,
                1 - (embedding <=> ${queryEmbedding}::vector) as similarity
            FROM "KnowledgeBaseChunk"
            WHERE knowledge_base_id = ${knowledgeBaseId}
            ORDER BY similarity DESC
            LIMIT ${limit}
        `;

        return relevantChunks
            .map(chunk => chunk.content)
            .join('\n\n');
    } catch (error) {
        console.error('Erro ao buscar contexto relevante:', error);
        return '';
    }
};

// Fun√ß√£o auxiliar para gerar embeddings
const getEmbedding = async (text: string): Promise<number[]> => {
    const response = await openai.embeddings.create({
        model: "text-embedding-3-small",
        input: text
    });
    return response.data[0].embedding;
};

// Fun√ß√µes auxiliares existentes
const processKnowledgeBaseContent = async (content: string): Promise<string> => {
    const sections = content.split(/\n{2,}/);
    let processedContent = '';

    // Processar cada se√ß√£o mantendo termos t√©cnicos e estrutura
    for (const section of sections) {
        // Identificar e preservar termos t√©cnicos
        const technicalTerms = extractTechnicalTerms(section);
        // Criar resumo mantendo contexto
        const sectionSummary = await summarizeSection(section, technicalTerms);
        processedContent += sectionSummary + '\n\n';
    }

    return processedContent.trim();
};

const extractTechnicalTerms = (text: string): string[] => {
    // Padr√µes para identificar termos t√©cnicos (customizar conforme necessidade)
    const patterns = [
        /[A-Z][a-z]+(?:[A-Z][a-z]+)*/g,  // CamelCase
        /\b[A-Z]+\b/g,                    // Siglas
        /\b[A-Z][a-z]+\b/g,              // Palavras capitalizadas
        /\b\w+(?:[-_]\w+)+\b/g           // Termos com h√≠fen ou underscore
    ];

    const terms = new Set<string>();
    patterns.forEach(pattern => {
        const matches = text.match(pattern) || [];
        matches.forEach(term => terms.add(term));
    });

    return Array.from(terms);
};

const summarizeSection = async (section: string, technicalTerms: string[]): Promise<string> => {
    // Preservar termos t√©cnicos no resumo
    const termsList = technicalTerms.join(', ');
    const prompt = `
        Resumo da se√ß√£o mantendo os seguintes termos t√©cnicos: ${termsList}
        
        Se√ß√£o original:
        ${section}
        
        Por favor, crie um resumo conciso que:
        1. Mantenha todos os termos t√©cnicos mencionados
        2. Preserve o contexto principal
        3. Mantenha a estrutura essencial
    `;

    try {
        const response = await openai.chat.completions.create({
            model: "gpt-4",
            messages: [{ role: "user", content: prompt }],
            temperature: 0.3
        });

        return response.choices[0].message.content || section;
    } catch (error) {
        console.error('Erro ao resumir se√ß√£o:', error);
        return section; // Em caso de erro, retorna a se√ß√£o original
    }
};

// Fun√ß√£o para dividir texto em chunks significativos
const splitIntoChunks = (text: string): string[] => {
    const paragraphs = text.split(/\n\n+/);
    const chunks: string[] = [];
    let currentChunk = '';

    for (const paragraph of paragraphs) {
        if ((currentChunk + paragraph).length > 1000) {
            if (currentChunk) chunks.push(currentChunk.trim());
            currentChunk = paragraph;
        } else {
            currentChunk += (currentChunk ? '\n\n' : '') + paragraph;
        }
    }

    if (currentChunk) chunks.push(currentChunk.trim());
    return chunks;
};